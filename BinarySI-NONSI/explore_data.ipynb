{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pandas import DataFrame\n",
    "import glob\n",
    "from collections import Counter\n",
    "import re\n",
    "from sklearn.feature_extraction import text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = '/Users/thuong/Documents/tmp_datasets/SI/SI-NonSI'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NonSI', 'SI']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dirs = next(os.walk(data_dir))[1]\n",
    "dirs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class_names = []\n",
    "total_amount = []\n",
    "train_amount = []\n",
    "test_amount = []\n",
    "\n",
    "train_data = DataFrame({'value': [], 'class': []})\n",
    "test_data = DataFrame({'value': [], 'class': []})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of example: 269\n"
     ]
    }
   ],
   "source": [
    "rows = []\n",
    "index = []\n",
    "for d in dirs:\n",
    "    tmp_dir = os.path.join(data_dir, d)\n",
    "    for f in glob.glob(os.path.join(tmp_dir, '*.txt')):\n",
    "        with open(f, encoding=\"utf-8\") as fc:\n",
    "            value = [line.replace('\\n', '').replace('\\r', '').replace('\\t', '') \n",
    "                        for line in fc.readlines()]\n",
    "            value = ' '.join(value)\n",
    "            rows.append({'value': value, 'class': d})\n",
    "            index.append(f)\n",
    "            \n",
    "tmp_df = DataFrame(rows, index=index)\n",
    "print(\"Total number of example: %s\" % len(tmp_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(text.ENGLISH_STOP_WORDS)\n",
    "for d in dirs:\n",
    "    tmp_dir = os.path.join(data_dir, d)\n",
    "    all_words = []\n",
    "    for f in glob.glob(os.path.join(tmp_dir, '*.txt')):\n",
    "        with open(f, encoding=\"utf-8\") as fc:\n",
    "            for line in fc.readlines():\n",
    "                all_words.extend(w for w in line.split() \n",
    "                                if w.isalpha() and len(w) != 1 and w.lower() not in stop_words)\n",
    "dictionary = Counter(all_words)\n",
    "dictionary = dictionary.most_common()\n",
    "dict = []\n",
    "for i, v in enumerate(dictionary):\n",
    "    if v[1] > 1:\n",
    "        dict.append(v)\n",
    "# print(dict)\n",
    "print(\"Number of words: %s\" % len(dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Note on stopwords\n",
    "# https://stackoverflow.com/questions/24386489/adding-words-to-scikit-learns-countvectorizers-stop-list\n",
    "# stop_words = text.ENGLISH_STOP_WORDS.union(my_additional_stop_words)\n",
    "# https://stackoverflow.com/questions/19130512/stopword-removal-with-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# print(tmp_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in SI: 16090\n"
     ]
    }
   ],
   "source": [
    "SI_dir = '/Users/thuong/Documents/tmp_datasets/SI/SI-NonSI/SI'\n",
    "all_words = []\n",
    "for f in glob.glob(os.path.join(SI_dir, '*.txt')):\n",
    "    with open(f, encoding=\"utf-8\") as fc:\n",
    "        for line in fc.readlines():\n",
    "            all_words += line.split()\n",
    "dictionary = Counter(all_words)\n",
    "print(\"Number of words in SI: %s\" % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in NON-SI: 5470\n"
     ]
    }
   ],
   "source": [
    "NonSI_dir = '/Users/thuong/Documents/tmp_datasets/SI/SI-NonSI/NonSI'\n",
    "all_words = []\n",
    "for f in glob.glob(os.path.join(NonSI_dir, '*.txt')):\n",
    "    with open(f, encoding=\"utf-8\") as fc:\n",
    "        for line in fc.readlines():\n",
    "            all_words += line.split()\n",
    "dictionary = Counter(all_words)\n",
    "print(\"Number of words in NON-SI: %s\" % len(dictionary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "a = \"firrd0\"\n",
    "# bool(re.search(r'\\d', a))\n",
    "a.isalpha()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import text\n",
    "stop_words = text.ENGLISH_STOP_WORDS\n",
    "# print((stop_words))\n",
    "# print(len(stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(term, doc, docset):\n",
    "\t# tf = float(doc.count(term))/sum(w.count(term) for w in docset)\n",
    "\ttf = float(doc.count(term)) / sum(doc.count(w) for w in set(doc)) \n",
    "\t#tf = float(doc.count(term))/len(doc)\n",
    "\tidf = math.log(float(len(docset))/(len([doc for doc in docset if term in doc])))\n",
    "\treturn tf*idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "\n",
    "texts=[\"dog cat fish\",\"dog cat cat\",\"fish bird\", 'bird cat']\n",
    "cv = CountVectorizer()\n",
    "cv_fit=cv.fit_transform(texts)\n",
    "cv_arr = cv_fit.toarray()\n",
    "words = cv.get_feature_names()\n",
    "freq_on_corpus = cv_arr.sum(axis=0)\n",
    "num_doc_contain_word = np.count_nonzero(cv_arr, axis = 0)\n",
    "print(words)\n",
    "print(freq_on_corpus)\n",
    "print(num_doc_contain_word)\n",
    "\n",
    "matrix = np.transpose(np.array([words, freq_on_corpus, num_doc_contain_word]))\n",
    "print(matrix)\n",
    "\n",
    "df = DataFrame(matrix, columns=[\"word\", \"freq_on_corpus\", \"num_doc_contain_word\"])\n",
    "df\n",
    "\n",
    "a = np.array([[0, 1, 2], [3, 4, 5]])\n",
    "np.savetxt(\"test.txt\", matrix, fmt='%1s', delimiter=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "im = Image.open('embedded_01_resize.png')\n",
    "im = (np.array(im))\n",
    "\n",
    "r = im[:, :, 0].flatten()\n",
    "g = im[:, :, 1].flatten()\n",
    "b = im[:, :, 2].flatten()\n",
    "label = [0]\n",
    "\n",
    "out = np.array(list(label) + list(r) + list(g) + list(b), np.uint8)\n",
    "out.tofile(\"data_batch_1.bin\")\n",
    "print(out)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3(udcbot)",
   "language": "python",
   "name": ".udcbot"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
